# -*- coding: utf-8 -*-
"""Predicting Student Dropout.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yA2-rryvvIeKj-tbRQ-9o2rjYXTJC2o7

1. IMPORTING THE DEPENDENCIES
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from google.colab import drive
drive.mount('/content/drive')
# Mounting the drive

"""2. DATA LAODING AND UNDERSTANDING"""

# read the csv data to a pandas dataframe
df = pd.read_csv("/content/student dropout.csv")

"""Initial Encoding"""

df.shape
# It gives the dimension of the dataset ; how many rows and columns exist

df.head()
# It is used in Python with pandas to display the first 5 rows of a DataFrame df.

df.tail()
# It is used in Python with pandas to display the last 5 rows of a DataFrame df

df.info()
# Used to list out all the columns of the dataset
# Tells the number of non null entries in each row
# Also tells the datatype of each column

df.isnull().sum()
# It gives us the sum of all the null values present in the dataset, COLUMN-WISE

"""Check all the unique values in the categorical columns and analyses it.... as it avoids Avoid Overfitting â€¢ Reduce Model Complexity"""

for col in df.columns:
  extensive_features = ["Age", "Number_of_Absences", "Grade_1", "Grade_2", "Final_Grade"]
  if col not in extensive_features:
    print(col, df[col].unique())
    print("-"*50)

#Loops through each column name in the DataFrame df.
# It prints the unique values of all non-numerical columns in a pandas DataFrame.
#print(col, df[col].unique()):Prints the column name and all its unique values.
#Prints a separator line of 50 dashes (-) for clarity.

df.duplicated().sum()

df. describe()

# taget class distribution
df["Dropped_Out"].value_counts()

#huge class imbalance in the class distribution

"""**INSIGHTS :**

1. Checked for the datatype of each column
2. No missing values in any column
3. No dulpicate data/row
4. Checked for the number of unique values in each column
5. Identified class imabalance in target column

3. EXPLORATORY DATA ANALYSIS
"""

df.columns

"""**Univariate Analysis**

Numerical Columns

*   Age
*   Number_of_Absences
*   Grade_1
*   Grade_2
*   Final_Grade

Distribution Plots
"""

# Histogram for "age"

sns.histplot(df["Age"],kde=True)
plt.title("Age Distribution")

# Calculate mean and median of age
mean_age = df["Age"].mean()
median_age = df["Age"].median()

# Print values
print("Mean age:", mean_age)
print("Median age:", median_age)

# Draw vertical lines for mean and median on the plot
plt.axvline(mean_age, color="red", linestyle="--", label="Mean")
plt.axvline(median_age, color="green", linestyle="-", label="Median")

# Show the legend and the plot
plt.legend()
plt.show()

#plt.axvline:This draws a vertical line on your plot at position x (on the x-axis).
# density plotâ€”to show where the mean and median values of the "age" column lie.

# Histogram for "Number_of_Absences"

sns.histplot(df["Number_of_Absences"], kde=True)
plt.title("Distribution of Absences")

# calculate mean and median
absences_mean = df["Number_of_Absences"].mean()
absences_median = df["Number_of_Absences"].median()

print("Mean:", absences_mean)
print("Median:", absences_median)


# add vertical lines for mean and median
plt.axvline(absences_mean, color="red", linestyle="--", label="Mean")
plt.axvline(absences_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "Grade_1"

sns.histplot(df["Grade_1"], kde=True)
plt.title("Distribution of First Grade")

# calculate mean and median
grade_1_mean = df["Grade_1"].mean()
grade_1_median = df["Grade_1"].median()

print("Mean:", grade_1_mean)
print("Median:", grade_1_median)


# add vertical lines for mean and median
plt.axvline(grade_1_mean, color="red", linestyle="--", label="Mean")
plt.axvline(grade_1_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "Grade_2"

sns.histplot(df["Grade_2"], kde=True)
plt.title("Distribution of Second Grade")

# calculate mean and median
grade_2_mean = df["Grade_1"].mean()
grade_2_median = df["Grade_1"].median()

print("Mean:", grade_2_mean)
print("Median:", grade_2_median)


# add vertical lines for mean and median
plt.axvline(grade_2_mean, color="red", linestyle="--", label="Mean")
plt.axvline(grade_2_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

# Histogram for "Final_Grade"

sns.histplot(df["Final_Grade"], kde=True)
plt.title("Distribution of Final Grade")

# calculate mean and median
final_grade_mean = df["Final_Grade"].mean()
final_grade_median = df["Final_Grade"].median()

print("Mean:", final_grade_mean)
print("Median:", final_grade_median)


# add vertical lines for mean and median
plt.axvline(final_grade_mean, color="red", linestyle="--", label="Mean")
plt.axvline(final_grade_median, color="green", linestyle="-", label="Median")

plt.legend()

plt.show()

"""Boxplot for identifying outliers in the numerical columns"""

# box plot for age
sns.boxplot(x=df["Age"])
plt.title("Box Plot for Age")
plt.xlabel("Age")
plt.show()

# box plot for absences
sns.boxplot(x=df["Number_of_Absences"])
plt.title("Box Plot for Absences")
plt.xlabel("Absences")
plt.show()

# box plot for Grade 1
sns.boxplot(x=df["Grade_1"])
plt.title("Box Plot for Grade 1")
plt.xlabel("Grade 1")
plt.show()

# box plot for Grade_2
sns.boxplot(x=df["Grade_2"])
plt.title("Box Plot for Grade 2")
plt.xlabel("Grade 2")
plt.show()

# box plot for Final_Grade
sns.boxplot(x=df["Final_Grade"])
plt.title("Box Plot for Final Grade")
plt.xlabel("Final Grade")
plt.show()

"""Counting the number of Outliers"""

# count the outliers in AGE using IQR method
Q1 = df["Age"].quantile(0.25)
Q3 = df["Age"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
age_outliers = df[(df["Age"] < lower_bound) | (df["Age"] > upper_bound)]

#The value is less than the lower bound, or The value is greater than the upper bound

len(age_outliers)

# count the outliers in ABSENCES using IQR method
Q1 = df["Number_of_Absences"].quantile(0.25)
Q3 = df["Number_of_Absences"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
absences_outliers = df[(df["Number_of_Absences"] < lower_bound) | (df["Number_of_Absences"] > upper_bound)]

#The value is less than the lower bound, or The value is greater than the upper bound

len(absences_outliers)

# count the outliers in FIRST GRADE using IQR method
Q1 = df["Grade_1"].quantile(0.25)
Q3 = df["Grade_1"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
grade_1_outliers = df[(df["Grade_1"] < lower_bound) | (df["Grade_1"] > upper_bound)]

#The value is less than the lower bound, or The value is greater than the upper bound

len(grade_1_outliers)

# count the outliers in SECOND GRADE using IQR method
Q1 = df["Grade_2"].quantile(0.25)
Q3 = df["Grade_2"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
grade_2_outliers = df[(df["Grade_2"] < lower_bound) | (df["Grade_2"] > upper_bound)]

#The value is less than the lower bound, or The value is greater than the upper bound

len(grade_2_outliers)

# count the outliers in FINAL GRADE using IQR method
Q1 = df["Final_Grade"].quantile(0.25)
Q3 = df["Final_Grade"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
final_grade_outliers = df[(df["Final_Grade"] < lower_bound) | (df["Final_Grade"] > upper_bound)]

#The value is less than the lower bound, or The value is greater than the upper bound

len(final_grade_outliers)

"""Handling Outliers"""

#AGE
# Step: Find the median
median_age = df["Age"].median()

# Step: Replace outliers with the median
df.loc[df["Age"] < lower_bound, "Age"] = median_age
df.loc[df["Age"] > upper_bound, "Age"] = median_age

print("Outliers in age column have been replaced with the median.")

#df.loc[...]:Access rows in the DataFrame by condition
#It finds all the rows in the DataFrame where the "age" is less than the lower bound (i.e., too small), and replaces those values with the median.

# Calculate IQR bounds for Number_of_Absences
Q1 = df["Number_of_Absences"].quantile(0.25)
Q3 = df["Number_of_Absences"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# we have calculated this again coz if not done , the upper limit and lower limit would come from age column

# Median for Number_of_Absences
median_absences = df["Number_of_Absences"].median()

# Replace outliers with median
df.loc[df["Number_of_Absences"] < lower_bound, "Number_of_Absences"] = median_absences
df.loc[df["Number_of_Absences"] > upper_bound, "Number_of_Absences"] = median_absences

print("Outliers in Number_of_Absences column have been replaced with the median.")

# Calculate IQR bounds for Grade_1
Q1 = df["Grade_1"].quantile(0.25)
Q3 = df["Grade_1"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Median for Grade_1
median_grade_1 = df["Grade_1"].median()

# Replace outliers with median
df.loc[df["Grade_1"] < lower_bound, "Grade_1"] = median_grade_1
df.loc[df["Grade_1"] > upper_bound, "Grade_1"] = median_grade_1

print("Outliers in Grade_1 column have been replaced with the median.")

# Calculate IQR bounds for Grade_2
Q1 = df["Grade_2"].quantile(0.25)
Q3 = df["Grade_2"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Median for Grade_2
median_grade_2 = df["Grade_2"].median()

# Replace outliers with median
df.loc[df["Grade_2"] < lower_bound, "Grade_2"] = median_grade_2
df.loc[df["Grade_2"] > upper_bound, "Grade_2"] = median_grade_2

print("Outliers in Grade_2 column have been replaced with the median.")

# Calculate IQR bounds for Final_Grade
Q1 = df["Final_Grade"].quantile(0.25)
Q3 = df["Final_Grade"].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Median for Final_Grade
median_final_grade = df["Final_Grade"].median()

# Replace outliers with median
df.loc[df["Final_Grade"] < lower_bound, "Final_Grade"] = median_final_grade
df.loc[df["Final_Grade"] > upper_bound, "Final_Grade"] = median_final_grade

print("Outliers in Final_Grade column have been replaced with the median.")

"""Univariate Ananlysis of Categorical Columns"""

df.columns

categorical_columns = ['School', 'Gender', 'Address', 'Parental_Status', 'Mother_Education', 'Father_Education',
       'Mother_Job', 'Father_Job', 'Reason_for_Choosing_School', 'Guardian', 'Travel_Time',
       'Study_Time', 'School_Support', 'Family_Support', 'Extra_Paid_Class', 'Extra_Curricular_Activities',
       'Attended_Nursery','Wants_Higher_Education','Internet_Access','In_Relationship','Family_Relationship','Free_Time',
       'Going_Out','Weekend_Alcohol_Consumption','Weekday_Alcohol_Consumption','Health_Status']

for col in categorical_columns:
  sns.countplot(x=df[col])
  plt.title(col)
  plt.xlabel(col)
  plt.ylabel("Count")
  plt.show()


  #is used to create count plots (bar plots) for all categorical columns in a dataset
  # using Seaborn and Matplotlib.
  #for col in categorical_columns:	Goes through each column name in your categorical_columns list

"""Label Encoding"""

# identify columns with "object" data type
object_columns = df.select_dtypes(include=["object"]).columns

#It finds all the columns in your DataFrame (df) that have data type object,
#and saves their names in a list called object_columns.
#object means non-numeric data â€” usually strings (like names, categories, yes/no, etc.)
#These are typically the categorical columns you will need to encode before using in ML model

print(object_columns)

from sklearn.preprocessing import LabelEncoder

# initialize a dictionary to store the encoders
encoders = {}

# apply label encoding and store the encoders
for column in object_columns:
  label_encoder = LabelEncoder()
  df[column] = label_encoder.fit_transform(df[column])
  encoders[column] = label_encoder   # saving the encoder for this column


#encoders = {}	Create an empty dictionary to store label encoders for each column
#encoders[column] = label_encoder	Save the encoder in the dictionary with column name as key

encoders

#encoders is a Python dictionary that stores the
# LabelEncoder objects for each categorical column in your DataFrame.

for column, encoder in encoders.items():
    print(f"\n Encoding for column: {column}")
    for idx, label in enumerate(encoder.classes_):
        print(f"  {label} â†’ {idx}")

#This code displays the label encoding mapping for each categorical column that was encoded using LabelEncoder
#Once it's fitted using .fit() or .fit_transform(), a LabelEncoder object stores: classes_ (most important attribute)
#This is an array of the unique classes (labels) it has learned, sorted in lexicographical order.
#encoder.classes_ is an attribute of LabelEncoder that contains the original string labels (before encoding), in sorted order.
#enumerate() gives you both: idx: the integer code assigned to the label, label: the original category value (e.g., "Male" or "Female")

df.head()

# display all columns of a dataframe
pd.set_option('display.max_columns', None)

#When you print a DataFrame with many columns, pandas by default hides some of them and shows ... in the middle.
#To see all columns without truncation
# None tells Pandas: "Show all columns, no matter how many there are."

df.sample()

"""Bivariate Analysis"""

# correlation matrix
plt.figure(figsize=(25, 25))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation heatmap")
plt.show()

#df.corr():Calculates the correlation matrix for all numeric columns in your dataset.
#Tells how strongly each column is related to the others.

# Get correlation of all features with the target column
correlation_with_target = df.corr(numeric_only=True)['Dropped_Out']

correlation_with_target

# Show features with weak correlation (less than 0.1)
weak_features = correlation_with_target[correlation_with_target.abs() < 0.01]

#features that have a weak correlation with the target â€” specifically
#features whose absolute correlation is less than 0.1.

print("Features with very weak or no correlation with Class/ASD:\n")
print(weak_features)
len(weak_features)

# Step 2: Drop those columns from the DataFrame
df = df.drop(columns=weak_features.index)

df.shape

# Drop the target itself from the list
# correlation_with_target = correlation_with_target.drop("Dropped_Out")
#### already done

df.shape

df["Dropped_Out"].value_counts()

"""**INSIGHTS FROM EDA**


1.   There are outliers in numerical columns (age, number of asbsences, first grade, second grade and final grade)
2.   There is class imbalance in target column
3. There is class imbalance in categorical columns
4. Found out the correlation of each feature with reespect to the target
5. Performed label encoding and saved the encoder


"""

df.columns

# defining features and target column
x = df.drop(columns=["Dropped_Out"])
y = df["Dropped_Out"]

x

y

#Split the data into training and testing set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)

x_train.shape

print(y_train.shape)
print(y_test.shape)

y_train.value_counts()

y_test.value_counts()

"""4. SPLITTING TRAINING AND TESTING SET

**SMOTE (Synthetic Minority Oversampling technique)**

Formula:

New Point=A+Î»Ã—(Bâˆ’A)

Where ðœ† Î» is a random number between 0 and 1
"""

smote = SMOTE(random_state=42)

#random_state=42:So every time you run the code, you get the same result

x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)

#.fit_resample(X_train, y_train)	Applies SMOTE only on the training data

print(x_train_smote.shape)

print(y_train_smote.shape)

print(y_train_smote.value_counts())

# scaling the numerical columns
from sklearn.preprocessing import StandardScaler

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(x_train_smote)
X_test_scaled = scaler.transform(x_test)

"""5. MODEL TRAINING



"""

# dictionary of classifiers
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42)
}

# Step 2: Create an empty dictionary to store scores
cv_scores = {}

# Step 3: Train and evaluate each model using 5-fold cross-validation
for name, model in models.items():

    # Perform 5-fold cross-validation using accuracy as the metric
    scores = cross_val_score(model, x_train_smote, y_train_smote, cv=5, scoring="accuracy")

    # Save scores in the dictionary
    cv_scores[name] = scores

    # Print average accuracy
    print(name, "Average Accuracy:", round(np.mean(scores), 4))

#It goes through each model in your models dictionary
#name is the model name as a string
#model is the actual model object
#for name, model in models.items():
#is a loop that:
#Assigns name = the key (e.g., "Decision Tree")
#Assigns model = the value (e.g., DecisionTreeClassifier())

cv_scores

"""6. MODEL SELECTION AND HYPER-PARAMETER TUNING"""

# Initializing models
decision_tree = DecisionTreeClassifier(random_state=42)
random_forest = RandomForestClassifier(random_state=42)
xgboost_classifier = XGBClassifier(random_state=42)

# Hyperparameter grids for RandomizedSearchCV

param_grid_dt = {
    "criterion": ["gini", "entropy"],
    "max_depth": [None, 10, 20, 30, 50, 70],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}


param_grid_rf = {
    "n_estimators": [50, 100, 200, 500],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "bootstrap": [True, False]
}


param_grid_xgb = {
    "n_estimators": [50, 100, 200, 500],
    "max_depth": [3, 5, 7, 10],
    "learning_rate": [0.01, 0.1, 0.2, 0.3],
    "subsample": [0.5, 0.7, 1.0],
    "colsample_bytree": [0.5, 0.7, 1.0]
}

#learning_rate	[0.01, 0.1, 0.2, 0.3]	Controls how much the model learns with each tree (lower = slower, safer)
#subsample	[0.5, 0.7, 1.0]	% of rows used per tree (1.0 = all rows; less = more randomness)
#colsample_bytree	[0.5, 0.7, 1.0]	% of features used per tree (adds randomness to reduce overfitting)

# hyperparameter tuning for 3 tree based models

# the below steps can be automated by using a for loop or by using a pipeline

# perform RandomizedSearchCV for each model
random_search_dt = RandomizedSearchCV(estimator=decision_tree, param_distributions=param_grid_dt, n_iter=20, cv=5, scoring="accuracy", random_state=42)
random_search_rf = RandomizedSearchCV(estimator=random_forest, param_distributions=param_grid_rf, n_iter=20, cv=5, scoring="accuracy", random_state=42)
random_search_xgb = RandomizedSearchCV(estimator=xgboost_classifier, param_distributions=param_grid_xgb, n_iter=20, cv=5, scoring="accuracy", random_state=42)


#n_iter=20	The number of random combinations to try. Instead of checking all combinations (as in GridSearchCV), it will randomly sample 20 of them.

# fit the models
random_search_dt.fit(x_train_smote, y_train_smote)
random_search_rf.fit(x_train_smote, y_train_smote)
random_search_xgb.fit(x_train_smote, y_train_smote)

random_search_dt.best_estimator_

random_search_dt.best_score_

random_search_rf.best_estimator_

random_search_rf.best_score_

random_search_xgb.best_estimator_

random_search_xgb.best_score_

# Get the model with best score

best_model = None
best_score = 0

#best_model: will hold the best-performing model
#best_score: starts with 0 and will be updated with the highest score


if random_search_dt.best_score_ > best_score:
  best_model = random_search_dt.best_estimator_
  best_score = random_search_dt.best_score_

if random_search_rf.best_score_ > best_score:
  best_model = random_search_rf.best_estimator_
  best_score = random_search_rf.best_score_

if random_search_xgb.best_score_ > best_score:
  best_model = random_search_xgb.best_estimator_
  best_score = random_search_xgb.best_score_


#random_search_dt.best_score_ (0.85) > best_score (0):
#Yes â†’ Set best_model = random_search_dt.best_estimator_, best_score = 0.85
#random_search_rf.best_score_ (0.919) > best_score (0.85):
#Yes â†’ Set best_model = random_search_rf.best_estimator_, best_score = 0.919
#random_search_xgb.best_score_ (0.90) > best_score (0.919):
#No: Set best_model = random_search_rf.best_estimator_, best_score = 0.919

print(f"Best Model: {best_model}")
print(f"Best Cross-Validation Accuracy: {best_score:.2f}")

"""7. Evaluation"""

# evaluate on test data
y_test_pred = best_model.predict(x_test)
print("Accuracy score:\n", accuracy_score(y_test, y_test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_test_pred))
print("Classification Report:\n", classification_report(y_test, y_test_pred))

# Plot the confusion matrix using seaborn heatmap
# Create confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True,fmt='d', cmap='Blues', xticklabels=["Non-ASD", "ASD"], yticklabels=["Non-ASD", "ASD"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Heatmap")
plt.show()